{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HumanActivityRecognition\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "This project is to build a model that predicts the human activities such as Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing or Laying.\n",
    "\n",
    "This dataset is collected from 30 persons(referred as subjects in this dataset), performing different activities with a smartphone to their waists. The data is recorded with the help of sensors (accelerometer and Gyroscope) in that smartphone. This experiment was video recorded to label the data manually.\n",
    "\n",
    "## How data was recorded\n",
    "\n",
    "By using the sensors(Gyroscope and accelerometer) in a smartphone, they have captured '3-axial linear acceleration'(_tAcc-XYZ_) from accelerometer and '3-axial angular velocity' (_tGyro-XYZ_) from Gyroscope with several variations. \n",
    "\n",
    "> prefix 't' in those metrics denotes time.\n",
    "\n",
    "> suffix 'XYZ' represents 3-axial signals in X , Y, and Z directions.\n",
    "\n",
    "### Feature names\n",
    "\n",
    "1. These sensor signals are preprocessed by applying noise filters and then sampled in fixed-width windows(sliding windows) of 2.56 seconds each with 50% overlap. ie., each window has 128 readings. \n",
    "\n",
    "2. From Each window, a feature vector was obtianed by calculating variables from the time and frequency domain.\n",
    "> In our dataset, each datapoint represents a window with different readings \n",
    "3. The accelertion signal was saperated into Body and Gravity acceleration signals(___tBodyAcc-XYZ___ and ___tGravityAcc-XYZ___) using some low pass filter with corner frequecy of 0.3Hz.\n",
    "\n",
    "4. After that, the body linear acceleration and angular velocity were derived in time to obtian _jerk signals_ (___tBodyAccJerk-XYZ___ and ___tBodyGyroJerk-XYZ___). \n",
    "\n",
    "5. The magnitude of these 3-dimensional signals were calculated using the Euclidian norm. This magnitudes are represented as features with names like _tBodyAccMag_, _tGravityAccMag_, _tBodyAccJerkMag_, _tBodyGyroMag_ and _tBodyGyroJerkMag_.\n",
    "\n",
    "6. Finally, We've got frequency domain signals from some of the available signals by applying a FFT (Fast Fourier Transform). These signals obtained were labeled with ___prefix 'f'___ just like original signals with ___prefix 't'___. These signals are labeled as ___fBodyAcc-XYZ___, ___fBodyGyroMag___ etc.,.\n",
    "\n",
    "7. These are the signals that we got so far.\n",
    "\t+ tBodyAcc-XYZ\n",
    "\t+ tGravityAcc-XYZ\n",
    "\t+ tBodyAccJerk-XYZ\n",
    "\t+ tBodyGyro-XYZ\n",
    "\t+ tBodyGyroJerk-XYZ\n",
    "\t+ tBodyAccMag\n",
    "\t+ tGravityAccMag\n",
    "\t+ tBodyAccJerkMag\n",
    "\t+ tBodyGyroMag\n",
    "\t+ tBodyGyroJerkMag\n",
    "\t+ fBodyAcc-XYZ\n",
    "\t+ fBodyAccJerk-XYZ\n",
    "\t+ fBodyGyro-XYZ\n",
    "\t+ fBodyAccMag\n",
    "\t+ fBodyAccJerkMag\n",
    "\t+ fBodyGyroMag\n",
    "\t+ fBodyGyroJerkMag\n",
    "\n",
    "8. We can esitmate some set of variables from the above signals. ie., We will estimate the following properties on each and every signal that we recoreded so far.\n",
    "\n",
    "\t+ ___mean()___: Mean value\n",
    "\t+ ___std()___: Standard deviation\n",
    "\t+ ___mad()___: Median absolute deviation \n",
    "\t+ ___max()___: Largest value in array\n",
    "\t+ ___min()___: Smallest value in array\n",
    "\t+ ___sma()___: Signal magnitude area\n",
    "\t+ ___energy()___: Energy measure. Sum of the squares divided by the number of values. \n",
    "\t+ ___iqr()___: Interquartile range \n",
    "\t+ ___entropy()___: Signal entropy\n",
    "\t+ ___arCoeff()___: Autorregresion coefficients with Burg order equal to 4\n",
    "\t+ ___correlation()___: correlation coefficient between two signals\n",
    "\t+ ___maxInds()___: index of the frequency component with largest magnitude\n",
    "\t+ ___meanFreq()___: Weighted average of the frequency components to obtain a mean frequency\n",
    "\t+ ___skewness()___: skewness of the frequency domain signal \n",
    "\t+ ___kurtosis()___: kurtosis of the frequency domain signal \n",
    "\t+ ___bandsEnergy()___: Energy of a frequency interval within the 64 bins of the FFT of each window.\n",
    "\t+ ___angle()___: Angle between to vectors.\n",
    "\n",
    "9. We can obtain some other vectors by taking the average of signals in a single window sample. These are used on the angle() variable'\n",
    "`\n",
    "\t+ gravityMean\n",
    "\t+ tBodyAccMean\n",
    "\t+ tBodyAccJerkMean\n",
    "\t+ tBodyGyroMean\n",
    "\t+ tBodyGyroJerkMean\n",
    "\n",
    "\n",
    "###  Y_Labels(Encoded)\n",
    "+ In the dataset, Y_labels are represented as numbers from 1 to 6 as their identifiers.\n",
    "\n",
    "\t- WALKING as __1__\n",
    "\t- WALKING_UPSTAIRS as __2__\n",
    "\t- WALKING_DOWNSTAIRS as __3__\n",
    "\t- SITTING as __4__\n",
    "\t- STANDING as __5__\n",
    "\t- LAYING as __6__\n",
    "    \n",
    "## Train and test data were saperated\n",
    " - The readings from ___70%___ of the volunteers were taken as ___trianing data___ and remaining ___30%___ subjects recordings were taken for ___test data___\n",
    " \n",
    "## Data\n",
    "\n",
    "* All the data is present in 'UCI_HAR_dataset/' folder in present working directory.\n",
    "     - Feature names are present in 'UCI_HAR_dataset/features.txt'\n",
    "     - ___Train Data___\n",
    "         - 'UCI_HAR_dataset/train/X_train.txt'\n",
    "         - 'UCI_HAR_dataset/train/subject_train.txt'\n",
    "         - 'UCI_HAR_dataset/train/y_train.txt'\n",
    "     - ___Test Data___\n",
    "         - 'UCI_HAR_dataset/test/X_test.txt'\n",
    "         - 'UCI_HAR_dataset/test/subject_test.txt'\n",
    "         - 'UCI_HAR_dataset/test/y_test.txt'\n",
    "         \n",
    "\n",
    "## Data Size :\n",
    "> 27 MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick overview of the dataset :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Accelerometer and Gyroscope readings are taken from 30 volunteers(referred as subjects) while performing the following 6 Activities.\n",
    "\n",
    "    1. Walking     \n",
    "    2. WalkingUpstairs \n",
    "    3. WalkingDownstairs \n",
    "    4. Standing \n",
    "    5. Sitting \n",
    "    6. Lying.\n",
    "\n",
    "\n",
    "* Readings are divided into a window of 2.56 seconds with 50% overlapping. \n",
    "\n",
    "* Accelerometer readings are divided into gravity acceleration and body acceleration readings,\n",
    "  which has x,y and z components each.\n",
    "\n",
    "* Gyroscope readings are the measure of angular velocities which has x,y and z components.\n",
    "\n",
    "* Jerk signals are calculated for BodyAcceleration readings.\n",
    "\n",
    "* Fourier Transforms are made on the above time readings to obtain frequency readings.\n",
    "\n",
    "* Now, on all the base signal readings., mean, max, mad, sma, arcoefficient, engerybands,entropy etc., are calculated for each window.\n",
    "\n",
    "* We get a feature vector of 561 features and these features are given in the dataset.\n",
    "\n",
    "* Each window of readings is a datapoint of 561 features.\n",
    "\n",
    "## Problem Framework\n",
    "\n",
    "* 30 subjects(volunteers) data is randomly split to 70%(21) test and 30%(7) train data.\n",
    "* Each datapoint corresponds one of the 6 Activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    " + Given a new datapoint we have to predict the Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T12:45:47.107008Z",
     "start_time": "2024-10-23T12:45:47.063494Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# get the features from the file features.txt\n",
    "features = list()\n",
    "with open('UCI_HAR_Dataset/features.txt') as f:\n",
    "    features = [line.split()[1] for line in f.readlines()]\n",
    "print('No of Features: {}'.format(len(features)))\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'UCI_HAR_Dataset/features.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# get the features from the file features.txt\u001B[39;00m\n\u001B[0;32m      5\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m()\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mUCI_HAR_Dataset/features.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m      7\u001B[0m     features \u001B[38;5;241m=\u001B[39m [line\u001B[38;5;241m.\u001B[39msplit()[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f\u001B[38;5;241m.\u001B[39mreadlines()]\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo of Features: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(features)))\n",
      "File \u001B[1;32m~\\Downloads\\TopicosAvancados2710\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'UCI_HAR_Dataset/features.txt'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the  train data "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T12:45:47.589966Z",
     "start_time": "2024-10-23T12:45:47.223086Z"
    }
   },
   "source": [
    "# get the data from txt files to pandas dataffame\n",
    "X_train = pd.read_csv('UCI_HAR_dataset/train/X_train.txt', delim_whitespace=True, header=None, names=features)\n",
    "\n",
    "# add subject column to the dataframe\n",
    "X_train['subject'] = pd.read_csv('UCI_HAR_dataset/train/subject_train.txt', header=None, squeeze=True)\n",
    "\n",
    "y_train = pd.read_csv('UCI_HAR_dataset/train/y_train.txt', names=['Activity'], squeeze=True)\n",
    "y_train_labels = y_train.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n",
    "                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n",
    "\n",
    "# put all columns in a single dataframe\n",
    "train = X_train\n",
    "train['Activity'] = y_train\n",
    "train['ActivityName'] = y_train_labels\n",
    "train.sample()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mique\\AppData\\Local\\Temp\\ipykernel_8460\\2724476813.py:2: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_train = pd.read_csv('UCI_HAR_dataset/train/X_train.txt', delim_whitespace=True, header=None, names=features)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'UCI_HAR_dataset/train/X_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# get the data from txt files to pandas dataffame\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mUCI_HAR_dataset/train/X_train.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelim_whitespace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# add subject column to the dataframe\u001B[39;00m\n\u001B[0;32m      5\u001B[0m X_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUCI_HAR_dataset/train/subject_train.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, squeeze\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Downloads\\TopicosAvancados2710\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\TopicosAvancados2710\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\Downloads\\TopicosAvancados2710\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\TopicosAvancados2710\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\Downloads\\TopicosAvancados2710\\venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'UCI_HAR_dataset/train/X_train.txt'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-23T12:45:47.592972Z",
     "start_time": "2024-10-23T12:45:47.592972Z"
    }
   },
   "source": [
    "train.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the  test data "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get the data from txt files to pandas dataffame\n",
    "X_test = pd.read_csv('UCI_HAR_dataset/test/X_test.txt', delim_whitespace=True, header=None, names=features)\n",
    "\n",
    "# add subject column to the dataframe\n",
    "X_test['subject'] = pd.read_csv('UCI_HAR_dataset/test/subject_test.txt', header=None, squeeze=True)\n",
    "\n",
    "# get y labels from the txt file\n",
    "y_test = pd.read_csv('UCI_HAR_dataset/test/y_test.txt', names=['Activity'], squeeze=True)\n",
    "y_test_labels = y_test.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n",
    "                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n",
    "\n",
    "\n",
    "# put all columns in a single dataframe\n",
    "test = X_test\n",
    "test['Activity'] = y_test\n",
    "test['ActivityName'] = y_test_labels\n",
    "test.sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print('No of duplicates in train: {}'.format(sum(train.duplicated())))\n",
    "print('No of duplicates in test : {}'.format(sum(test.duplicated())))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking for NaN/null values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('We have {} NaN/Null values in train'.format(train.isnull().values.sum()))\n",
    "print('We have {} NaN/Null values in test'.format(test.isnull().values.sum()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check for data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.family'] = 'Dejavu Sans'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Data provided by each user', fontsize=20)\n",
    "sns.countplot(x='subject',hue='ActivityName', data = train)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have got almost same number of reading from all the subjects"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.title('No of Datapoints per Activity', fontsize=15)\n",
    "sns.countplot(train.ActivityName)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "> Our data is well balanced (almost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Alterando nomes de recursos"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "columns = train.columns\n",
    "\n",
    "# Removing '()' from column names\n",
    "columns = columns.str.replace('[()]','')\n",
    "columns = columns.str.replace('[-]', '')\n",
    "columns = columns.str.replace('[,]','')\n",
    "\n",
    "train.columns = columns\n",
    "test.columns = columns\n",
    "\n",
    "test.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save this dataframe in a csv files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "train.to_csv('UCI_HAR_Dataset/csv_files/train.csv', index=False)\n",
    "test.to_csv('UCI_HAR_Dataset/csv_files/test.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"___Without domain knowledge EDA has no meaning, without EDA a problem has no soul.___\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Featuring Engineering from Domain Knowledge \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "+ __Static and Dynamic Activities__\n",
    "\n",
    "    - In static activities (sit, stand, lie down) motion information will not be very useful.\n",
    "\t- In the dynamic activities (Walking, WalkingUpstairs,WalkingDownstairs) motion info will be significant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stationary and Moving activities are completely different"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.set_palette(\"Set1\", desat=0.80)\n",
    "facetgrid = sns.FacetGrid(train, hue='ActivityName', size=6,aspect=2)\n",
    "facetgrid.map(sns.distplot,'tBodyAccMagmean', hist=False)\\\n",
    "    .add_legend()\n",
    "plt.annotate(\"Stationary Activities\", xy=(-0.956,17), xytext=(-0.9, 23), size=20,\\\n",
    "            va='center', ha='left',\\\n",
    "            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n",
    "\n",
    "plt.annotate(\"Moving Activities\", xy=(0,3), xytext=(0.2, 9), size=20,\\\n",
    "            va='center', ha='left',\\\n",
    "            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# for plotting purposes taking datapoints of each activity to a different dataframe\n",
    "df1 = train[train['Activity']==1]\n",
    "df2 = train[train['Activity']==2]\n",
    "df3 = train[train['Activity']==3]\n",
    "df4 = train[train['Activity']==4]\n",
    "df5 = train[train['Activity']==5]\n",
    "df6 = train[train['Activity']==6]\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Stationary Activities(Zoomed in)')\n",
    "sns.distplot(df4['tBodyAccMagmean'],color = 'r',hist = False, label = 'Sitting')\n",
    "sns.distplot(df5['tBodyAccMagmean'],color = 'm',hist = False,label = 'Standing')\n",
    "sns.distplot(df6['tBodyAccMagmean'],color = 'c',hist = False, label = 'Laying')\n",
    "plt.axis([-1.01, -0.5, 0, 35])\n",
    "plt.legend(loc='center')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Moving Activities')\n",
    "sns.distplot(df1['tBodyAccMagmean'],color = 'red',hist = False, label = 'Walking')\n",
    "sns.distplot(df2['tBodyAccMagmean'],color = 'blue',hist = False,label = 'Walking Up')\n",
    "sns.distplot(df3['tBodyAccMagmean'],color = 'green',hist = False, label = 'Walking down')\n",
    "plt.legend(loc='center right')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Magnitude of an acceleration can saperate it well"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "sns.boxplot(x='ActivityName', y='tBodyAccMagmean',data=train, showfliers=False, saturation=1)\n",
    "plt.ylabel('Acceleration Magnitude mean')\n",
    "plt.axhline(y=-0.7, xmin=0.1, xmax=0.9,dashes=(5,5), c='g')\n",
    "plt.axhline(y=-0.05, xmin=0.4, dashes=(5,5), c='m')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Observations__:\n",
    "- If tAccMean is < -0.8 then the Activities are either Standing or Sitting or Laying.\n",
    "- If tAccMean is > -0.6 then the Activities are either Walking or WalkingDownstairs or WalkingUpstairs.\n",
    "- If tAccMean > 0.0 then the Activity is WalkingDownstairs.\n",
    "- We can classify 75% the Acitivity labels with some errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Position of GravityAccelerationComponants also matters "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T12:45:47.653990Z",
     "start_time": "2024-10-23T12:45:47.626564Z"
    }
   },
   "source": [
    "sns.boxplot(x='ActivityName', y='angleXgravityMean', data=train)\n",
    "plt.axhline(y=0.08, xmin=0.1, xmax=0.9,c='m',dashes=(5,3))\n",
    "plt.title('Angle between X-axis and Gravity_mean', fontsize=15)\n",
    "plt.xticks(rotation = 40)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241m.\u001B[39mboxplot(x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActivityName\u001B[39m\u001B[38;5;124m'\u001B[39m, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mangleXgravityMean\u001B[39m\u001B[38;5;124m'\u001B[39m, data\u001B[38;5;241m=\u001B[39mtrain)\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39maxhline(y\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.08\u001B[39m, xmin\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, xmax\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m,c\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;124m'\u001B[39m,dashes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m3\u001B[39m))\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAngle between X-axis and Gravity_mean\u001B[39m\u001B[38;5;124m'\u001B[39m, fontsize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sns' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Observations__:\n",
    "* If angleX,gravityMean > 0 then Activity is Laying.\n",
    "* We can classify all datapoints belonging to Laying activity with just a single if else statement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T12:45:47.686112Z",
     "start_time": "2024-10-23T12:45:47.653990Z"
    }
   },
   "source": [
    "sns.boxplot(x='ActivityName', y='angleYgravityMean', data = train, showfliers=False)\n",
    "plt.title('Angle between Y-axis and Gravity_mean', fontsize=15)\n",
    "plt.xticks(rotation = 40)\n",
    "plt.axhline(y=-0.22, xmin=0.1, xmax=0.8, dashes=(5,3), c='m')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241m.\u001B[39mboxplot(x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActivityName\u001B[39m\u001B[38;5;124m'\u001B[39m, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mangleYgravityMean\u001B[39m\u001B[38;5;124m'\u001B[39m, data \u001B[38;5;241m=\u001B[39m train, showfliers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAngle between Y-axis and Gravity_mean\u001B[39m\u001B[38;5;124m'\u001B[39m, fontsize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m)\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mxticks(rotation \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m40\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sns' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply t-sne on the data "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-23T12:45:47.749662Z",
     "start_time": "2024-10-23T12:45:47.712794Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmanifold\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TSNE\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'sklearn'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# performs t-sne with different perplexity values and their repective plots..\n",
    "\n",
    "def perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n",
    "        \n",
    "    for index,perplexity in enumerate(perplexities):\n",
    "        # perform t-sne\n",
    "        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n",
    "        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n",
    "        print('Done..')\n",
    "        \n",
    "        # prepare the data for seaborn         \n",
    "        print('Creating plot for this t-sne visualization..')\n",
    "        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n",
    "        \n",
    "        # draw the plot in appropriate place in the grid\n",
    "        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\\\n",
    "                   palette=\"Set1\",markers=['^','v','s','o', '1','2'])\n",
    "        plt.title(\"perplexity : {} and max_iter : {}\".format(perplexity, n_iter))\n",
    "        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n",
    "        print('saving this plot as image in present working directory...')\n",
    "        plt.savefig(img_name)\n",
    "        plt.show()\n",
    "        print('Done')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T12:45:47.812839Z",
     "start_time": "2024-10-23T12:45:47.777729Z"
    }
   },
   "source": [
    "X_pre_tsne = train.drop(['subject', 'Activity','ActivityName'], axis=1)\n",
    "y_pre_tsne = train['ActivityName']\n",
    "perform_tsne(X_data = X_pre_tsne,y_data=y_pre_tsne, perplexities =[2,5,10,20,50])"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_pre_tsne \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[38;5;241m.\u001B[39mdrop([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActivity\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActivityName\u001B[39m\u001B[38;5;124m'\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      2\u001B[0m y_pre_tsne \u001B[38;5;241m=\u001B[39m train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActivityName\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      3\u001B[0m perform_tsne(X_data \u001B[38;5;241m=\u001B[39m X_pre_tsne,y_data\u001B[38;5;241m=\u001B[39my_pre_tsne, perplexities \u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m10\u001B[39m,\u001B[38;5;241m20\u001B[39m,\u001B[38;5;241m50\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
